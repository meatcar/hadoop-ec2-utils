import sys
import boto
import boto.ec2
import json
import random

from time import sleep

from node_manager import NodeManager

class ClusterLauncher(object):
    group_name = u'allspark'
    key_pair_name = None
    cluster_size = None

    def __init__(self, cluster_name='allspark',
                       instance_type='m1.small',
                       hdfs_node_size=8,
                       cluster_size=1,
                       key_pair_name = u'hadoop_allspark_key',
                       slaves=[],
                       volumes=[],
                       conf_file="conf.json"):

        self.cluster_name = cluster_name
        self.instance_type = instance_type
        self.key_pair_name = key_pair_name
        self.cluster_size = cluster_size

        with open(conf_file) as conf_json:
            self.conf = json.load(conf_json)

        self.conn = boto.ec2.connect_to_region(
                self.conf['region'],
                aws_access_key_id = self.conf['key'],
                aws_secret_access_key= self.conf['secret_key'])

        self.group = self.get_security_group()
        self.key_pair = self.get_key_pair()
        self.slaves = [self.conn.get_all_instances(i)[0] for i in slaves]
        self.volumes = [self.conn.get_all_volumes(i)[0] for i in volumes]

    def create_cluster(self):
        self.slaves = self.get_slaves(self.cluster_size)
        self.master = self.start_node(self.conf['allspark_ami'])

        self.wait_for_instance(self.master)
        print "waiting for ssh to be available (may take some time)..."
        sleep(60)

        for slave in self.slaves: # update all the slaves, make sure they're up
            self.wait_for_instance(slave)
        slave_names = [s.public_dns_name for s in self.slaves]

        master_nm = NodeManager(self.key_pair_name, self.master.public_dns_name)
        master_nm.copy_key()
        master_nm.add_cluster_config(
                slave_names,
                self.master.public_dns_name,
                self.master.ip_address)

        self.attach_volumes_to_slaves()

        master_nm.mount_volume()
        for slave in self.slaves:
            self.wait_for_instance(slave)
            nm = NodeManager(self.key_pair_name, slave.public_dns_name)
            nm.copy_key()
            nm.add_cluster_config(
                    slave_names,
                    self.master.public_dns_name,
                    self.master.ip_address)
            nm.mount_volume()

        master_nm.start_hadoop()

        self.dump_json([v.id for v in self.volumes], '%s-volumes.json' % self.cluster_name)


    def create_image(self):
        node = self.start_node(self.conf['ami'], suffix="image-staging")
        self.wait_for_instance(node)
        sleep(60) # wait for sshd to load

        nm = NodeManager(self.key_pair_name, node.public_dns_name)
        nm.setup()
        nm.get_hadoop()
        nm.setup_hadoop()

        node.stop()
        self.wait_for_instance(node, "stopped")
        self.conf['allspark_ami'] = node.create_image(
                "Hadoop Allspark Img %d" % random.randint(100, 999),
                "Hadoop Allspark node image")
        node.terminate()

        print "Waiting for ami to be available"
        ami = self.conn.get_all_images(self.conf['allspark_ami'])[0]
        while ami.update() != 'available':
            sleep(1)

        print "Created ami %s" % self.conf['allspark_ami']

    def cleanup(self, delete_volumes=False):
        for s in self.slaves:
            print "terminating %s %s" % (s.tags['Name'], s)
            s.terminate()

        print "terminating %s %s" % (self.master.tags['Name'], self.master)
        self.master.terminate()

        if delete_volumes:
            print "deleting volumes"
            for v in self.volumes:
                self.detach_vol(v)
                v.delete()

        self.conn.close()

    def get_slaves(self, num):
        if self.slaves is not None and len(self.slaves) > 0:
            return self.slaves

        slaves = []
        for i in xrange(0, num):
            slaves.append(self.start_node(ami=self.conf['allspark_ami'], slave=True))

        return slaves

    def get_security_group(self):
        groups = self.conn.get_all_security_groups()
        groups = [g for g in groups if g.name == self.group_name]

        if len(groups) == 1:
            return groups[0]

        group = self.conn.create_security_group(
                self.group_name, 'Hadoop Cluster Group (generated by script)')

        group.authorize('tcp', 22, 22, '0.0.0.0/0') #ssh
        group.authorize('tcp', 8020, 8040, '0.0.0.0/0') #hadoop/yarn
        group.authorize('tcp', 9000, 9000, '0.0.0.0/0') #hadoop/hdfs
        group.authorize('tcp', 50020, 50020, '0.0.0.0/0') #hadoop/hdfs
        group.authorize('tcp', 50075, 50075, '0.0.0.0/0') #hadoop/hdfs
        group.authorize('icmp', -1, -1, '0.0.0.0/0') #ssh

        return group

    def get_key_pair(self):
        keys = self.conn.get_all_key_pairs()
        keys = [k for k in keys if k.name == self.key_pair_name]

        if len(keys) == 1:
            return keys[0]
        print "creating key pair"

        key_pair = self.conn.create_key_pair(self.key_pair_name)
        key_pair.save('./')
        return key_pair

    def start_node(self, ami, slave=False, suffix=''):
        reservation = self.conn.run_instances(
                ami,
                instance_type='m1.small',
                key_name=self.key_pair_name,
                security_groups=[self.group_name]
                )

        if len(reservation.instances) != 1:
            print ("Didn't launce exactly one instance!")
            sys.exit(1)

        instance = reservation.instances[0]

        if len(suffix) > 0:
            name = "%s-%s" % (self.cluster_name, suffix)
        elif slave:
            name = '%s-slave' % self.cluster_name
        else:
            name = '%s-master' % self.cluster_name

        instance.add_tag('Name', value=name)

        print "starting %s, %s" %  (name, instance)
        return instance

    def wait_for_instance(self, instance, state="running"):
        '''
        Block until instance reaches desired state.
        '''

        print "waiting for %s" % (instance)
        while(instance.update() != state):
            print "..instance is %s" % instance.state
            sleep(1)

    def attach_volumes_to_slaves(self):
        if self.volumes is None:
            self.volumes = []

        if len(self.volumes) == 0:
            vol = self.conn.create_volume(
                    size=8,
                    zone=self.master.placement)
            self.volumes.append(vol)

        self.attach_vol_to_slave(self.volumes[0], self.master)

        slave_volumes = self.volumes[1:]
        for i in xrange(0, len(self.slaves)):
            slave = self.slaves[i]

            # TODO: if #slave < #vols, each slave gets #vols%#slaves volumes
            if i < len(slave_volumes):
                vol = slave_volumes[i]
            else:
                vol = self.conn.create_volume(
                        size=8,
                        zone=slave.placement)
                self.volumes.append(vol)

            self.attach_vol_to_slave(vol, slave)

    def attach_vol_to_slave(self, vol, slave):
        '''
        Attach a given volume to a given slave.
        Detaches the volume if in use.
        '''

        print "attaching %s to %s" % (vol, slave)

        self.detach_vol(vol)

        vol.attach(slave.id, '/dev/sdf')

        while vol.update() != 'in-use':
            sleep(1)

    def detach_vol(self, vol):
        '''
        Cleanly detach the volume, waiting through the intermediate states
        '''
        state = vol.update()

        while state != 'available' and state != 'in-use':
            sleep(1)
            state = vol.update()

        if state == 'in-use':
            vol.detach()

            while state != 'available':
                sleep(1)
                state = vol.update()

    def dump_json(self, data, filename):
        json_str = json.dumps(data,
                sort_keys=True,
                indent=4, separators=(',', ': '))

        with file(filename, 'w') as f:
            f.write(json_str)

if __name__ == "__main__":
    c = ClusterLauncher()
    c.create_image()
    print c.master.public_dns_name
    print [s.public_dns_name for s in c.slaves]
